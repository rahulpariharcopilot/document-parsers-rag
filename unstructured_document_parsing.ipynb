{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Parsing with Unstructured for RAG Systems\n",
    "\n",
    "This comprehensive notebook demonstrates how to use **Unstructured** (v0.18.15) for document parsing in Retrieval-Augmented Generation (RAG) systems. Unstructured is a powerful open-source library that provides tools for ingesting and pre-processing documents for use with Large Language Models.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Core Concepts**: Understanding partition functions, elements, and metadata\n",
    "2. **Partitioning Strategies**: AUTO, FAST, HI_RES, and OCR_ONLY approaches\n",
    "3. **File Format Support**: Working with PDFs, Office documents, HTML, Markdown, Images, and more\n",
    "4. **LangChain Integration**: Seamless integration with LangChain for RAG pipelines\n",
    "5. **Complete RAG Example**: End-to-end implementation with vector stores\n",
    "\n",
    "## Key Features of Unstructured\n",
    "\n",
    "- **28+ Supported File Formats**: PDF, DOCX, XLSX, PPTX, HTML, Markdown, Images, Email, and more\n",
    "- **Multiple Partitioning Strategies**: Choose the right balance between speed and accuracy\n",
    "- **Rich Element Types**: Title, NarrativeText, Table, Image, ListItem, etc.\n",
    "- **Comprehensive Metadata**: Page numbers, coordinates, file types, and custom fields\n",
    "- **OCR Support**: Built-in OCR for scanned documents and images\n",
    "- **LangChain Integration**: Native support through `langchain-unstructured`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Installation & Setup\n",
    "\n",
    "First, let's install all the required packages. Unstructured supports various \"extras\" for different file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unstructured with all common extras\n",
    "# The extras enable support for specific file formats\n",
    "\n",
    "# Core installation with common document format support\n",
    "#!uv pip install -q \"unstructured==0.18.15\"\n",
    "\n",
    "# Install extras for specific file formats\n",
    "# - pdf: PDF document support\n",
    "# - docx: Microsoft Word documents\n",
    "# - xlsx: Microsoft Excel spreadsheets\n",
    "# - pptx: Microsoft PowerPoint presentations\n",
    "# - html: HTML web pages\n",
    "# - md: Markdown files\n",
    "# - image: Image files (PNG, JPEG, etc.)\n",
    "# - email: Email files (EML, MSG)\n",
    "#!uv pip install -q \"unstructured[pdf,docx,xlsx,pptx,html,md,image,email]\"\n",
    "\n",
    "# Install LangChain integration packages\n",
    "#!uv pip install -q langchain-unstructured langchain-community langchain-openai\n",
    "\n",
    "# Install additional dependencies\n",
    "#!uv pip install -q chromadb python-dotenv\n",
    "\n",
    "#print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.21\n"
     ]
    }
   ],
   "source": [
    "# Verify installation and check version\n",
    "import unstructured\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(version(\"unstructured\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables for API keys\n",
    "# Create a .env file with your OPENAI_API_KEY\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify OpenAI API key is set (for RAG examples later)\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"OpenAI API key loaded successfully\")\n",
    "else:\n",
    "    print(\"Warning: OPENAI_API_KEY not found. RAG examples will not work.\")\n",
    "    print(\"Create a .env file with: OPENAI_API_KEY=your-key-here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import commonly used modules\n",
    "# These will be used throughout the notebook\n",
    "\n",
    "from unstructured.partition.auto import partition\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.partition.md import partition_md\n",
    "from unstructured.partition.docx import partition_docx\n",
    "from unstructured.partition.xlsx import partition_xlsx\n",
    "from unstructured.partition.pptx import partition_pptx\n",
    "from unstructured.partition.image import partition_image\n",
    "\n",
    "# Chunking utilities\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.chunking.basic import chunk_elements\n",
    "\n",
    "# Element types for type checking\n",
    "from unstructured.documents.elements import (\n",
    "    Title,\n",
    "    NarrativeText,\n",
    "    Table,\n",
    "    ListItem,\n",
    "    Image,\n",
    "    Header,\n",
    "    Footer,\n",
    "    Text,\n",
    "    ElementMetadata,\n",
    ")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Core Concepts\n",
    "\n",
    "### Understanding the `partition()` Function\n",
    "\n",
    "The `partition()` function is the universal entry point for document processing in Unstructured. It automatically detects the file type and applies the appropriate partitioning logic.\n",
    "\n",
    "### Element Types\n",
    "\n",
    "Unstructured extracts structured elements from documents:\n",
    "\n",
    "| Element Type | Description |\n",
    "|-------------|-------------|\n",
    "| `Title` | Headings and section titles |\n",
    "| `NarrativeText` | Paragraphs and body text |\n",
    "| `Table` | Tabular data |\n",
    "| `ListItem` | List elements (bulleted, numbered) |\n",
    "| `Image` | Embedded images |\n",
    "| `Header` | Page headers |\n",
    "| `Footer` | Page footers |\n",
    "| `Text` | Generic text elements |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elements extracted: 20\n",
      "\n",
      "============================================================\n",
      "\n",
      "[0] Title: 1. Overview\n",
      "------------------------------------------------------------\n",
      "[1] NarrativeText: Document parsing is the process of analyzing and extracting structured information from various docu...\n",
      "------------------------------------------------------------\n",
      "[2] Title: 1.1 Key Benefits\n",
      "------------------------------------------------------------\n",
      "[3] ListItem: Automated data extraction\n",
      "------------------------------------------------------------\n",
      "[4] ListItem: Structured content analysis\n",
      "------------------------------------------------------------\n",
      "[5] ListItem: Integration with AI/ML pipelines\n",
      "------------------------------------------------------------\n",
      "[6] ListItem: Support for multiple formats\n",
      "------------------------------------------------------------\n",
      "[7] Title: 2. Core Features\n",
      "------------------------------------------------------------\n",
      "[8] NarrativeText: Modern document parsers offer a variety of features:\n",
      "------------------------------------------------------------\n",
      "[9] Table: Feature Description Use Case OCR Support Optical Character Recognition for scanned documents Scanned...\n",
      "------------------------------------------------------------\n",
      "[10] Title: 3. Applications in RAG Systems\n",
      "------------------------------------------------------------\n",
      "[11] NarrativeText: Retrieval-Augmented Generation (RAG) systems benefit significantly from proper document parsing:\n",
      "------------------------------------------------------------\n",
      "[12] ListItem: Knowledge Base Creation: Convert documents into searchable chunks\n",
      "------------------------------------------------------------\n",
      "[13] ListItem: Semantic Search: Enable meaning-based document retrieval\n",
      "------------------------------------------------------------\n",
      "[14] ListItem: Question Answering: Provide accurate answers from document context\n",
      "------------------------------------------------------------\n",
      "[15] ListItem: Document Summarization: Generate concise summaries\n",
      "------------------------------------------------------------\n",
      "[16] NarrativeText: \"Effective document parsing is the foundation of any successful RAG implementation.\"\n",
      "------------------------------------------------------------\n",
      "[17] Title: 4. Code Example\n",
      "------------------------------------------------------------\n",
      "[18] NarrativeText: Here's a simple example of using a document parser:\n",
      "------------------------------------------------------------\n",
      "[19] NarrativeText: from docling.document_converter import DocumentConverter\n",
      "\n",
      "# Initialize the converter\n",
      "converter = Doc...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Basic example: Partition a local HTML file\n",
    "# Using the sample HTML file from the sample_documents folder\n",
    "\n",
    "html_path = \"sample_documents/sample.html\"\n",
    "\n",
    "# The partition() function automatically detects the file type\n",
    "# and applies the appropriate parsing logic\n",
    "elements = partition(filename=html_path)\n",
    "\n",
    "# Display the number of elements extracted\n",
    "print(f\"Total elements extracted: {len(elements)}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Show each element with its type\n",
    "for i, element in enumerate(elements):\n",
    "    element_type = type(element).__name__\n",
    "    # Truncate long text for display\n",
    "    text_preview = element.text[:100] + \"...\" if len(element.text) > 100 else element.text\n",
    "    print(f\"[{i}] {element_type}: {text_preview}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element Text:\n",
      "1. Overview\n",
      "\n",
      "============================================================\n",
      "\n",
      "Element Metadata:\n",
      "  category_depth: 1\n",
      "  last_modified: 2025-11-28T09:19:12\n",
      "  languages: ['eng']\n",
      "  file_directory: sample_documents\n",
      "  filename: sample.html\n",
      "  filetype: text/html\n"
     ]
    }
   ],
   "source": [
    "# Understanding ElementMetadata\n",
    "# Each element has rich metadata attached to it\n",
    "\n",
    "# Get the first Title element\n",
    "title_elements = [e for e in elements if isinstance(e, Title)]\n",
    "\n",
    "if title_elements:\n",
    "    first_title = title_elements[0]\n",
    "    \n",
    "    print(\"Element Text:\")\n",
    "    print(first_title.text)\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    print(\"Element Metadata:\")\n",
    "    metadata = first_title.metadata\n",
    "    \n",
    "    # Display available metadata fields\n",
    "    metadata_dict = metadata.to_dict()\n",
    "    for key, value in metadata_dict.items():\n",
    "        if value is not None:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element Distribution:\n",
      "------------------------------\n",
      "  ListItem: 8\n",
      "  NarrativeText: 6\n",
      "  Table: 1\n",
      "  Title: 5\n"
     ]
    }
   ],
   "source": [
    "# Filter elements by type\n",
    "# This is useful when you need specific content from a document\n",
    "\n",
    "def analyze_elements(elements):\n",
    "    \"\"\"Analyze and categorize extracted elements.\"\"\"\n",
    "    \n",
    "    # Count elements by type\n",
    "    type_counts = {}\n",
    "    for element in elements:\n",
    "        element_type = type(element).__name__\n",
    "        type_counts[element_type] = type_counts.get(element_type, 0) + 1\n",
    "    \n",
    "    print(\"Element Distribution:\")\n",
    "    print(\"-\" * 30)\n",
    "    for elem_type, count in sorted(type_counts.items()):\n",
    "        print(f\"  {elem_type}: {count}\")\n",
    "    \n",
    "    return type_counts\n",
    "\n",
    "# Analyze the HTML elements\n",
    "type_counts = analyze_elements(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 NarrativeText elements:\n",
      "\n",
      "[1] Document parsing is the process of analyzing and extracting structured information from various document formats. This includes PDFs, Word documents, HTML pages, and more.\n",
      "\n",
      "[2] Modern document parsers offer a variety of features:\n",
      "\n",
      "[3] Retrieval-Augmented Generation (RAG) systems benefit significantly from proper document parsing:\n",
      "\n",
      "[4] \"Effective document parsing is the foundation of any successful RAG implementation.\"\n",
      "\n",
      "[5] Here's a simple example of using a document parser:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract specific element types\n",
    "# Example: Get all narrative text for summarization\n",
    "\n",
    "narrative_texts = [e for e in elements if isinstance(e, NarrativeText)]\n",
    "\n",
    "print(f\"Found {len(narrative_texts)} NarrativeText elements:\\n\")\n",
    "for i, text_elem in enumerate(narrative_texts[:5]):  # Show first 5\n",
    "    print(f\"[{i+1}] {text_elem.text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 Table elements:\n",
      "\n",
      "Table 1:\n",
      "Feature Description Use Case OCR Support Optical Character Recognition for scanned documents Scanned PDFs, Images Table Extraction Structured table data extraction Financial reports, Data tables Layout Analysis Understanding document structure Academic papers, Legal documents Image Processing Extract and classify images Technical manuals, Presentations\n",
      "\n",
      "HTML representation available in metadata.text_as_html\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract tables from the document\n",
    "# Tables maintain their structure and can be exported to different formats\n",
    "\n",
    "table_elements = [e for e in elements if isinstance(e, Table)]\n",
    "\n",
    "print(f\"Found {len(table_elements)} Table elements:\\n\")\n",
    "\n",
    "for i, table in enumerate(table_elements):\n",
    "    print(f\"Table {i+1}:\")\n",
    "    print(table.text)\n",
    "    print()\n",
    "    \n",
    "    # If table has HTML representation in metadata\n",
    "    if hasattr(table.metadata, 'text_as_html') and table.metadata.text_as_html:\n",
    "        print(\"HTML representation available in metadata.text_as_html\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Partitioning Strategies\n",
    "\n",
    "Unstructured offers different partitioning strategies to balance speed and accuracy:\n",
    "\n",
    "| Strategy | Speed | Accuracy | Use Case |\n",
    "|----------|-------|----------|----------|\n",
    "| `auto` | Variable | Good | Default choice, selects best strategy |\n",
    "| `fast` | Fast | Basic | Quick processing, no models |\n",
    "| `hi_res` | Slow | Best | Complex layouts, OCR, tables |\n",
    "| `ocr_only` | Medium | For scanned | Purely OCR-based extraction |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF already exists at sample_documents/docling_paper.pdf\n"
     ]
    }
   ],
   "source": [
    "# Download a sample PDF for demonstrating different strategies\n",
    "# Using the Docling paper from arXiv as an example\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Create sample_documents directory if it doesn't exist\n",
    "os.makedirs(\"sample_documents\", exist_ok=True)\n",
    "\n",
    "pdf_url = \"https://arxiv.org/pdf/2408.09869\"\n",
    "pdf_path = \"sample_documents/docling_paper.pdf\"\n",
    "\n",
    "# Download only if not already present\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Downloading PDF from {pdf_url}...\")\n",
    "    urllib.request.urlretrieve(pdf_url, pdf_path)\n",
    "    print(f\"Downloaded to {pdf_path}\")\n",
    "else:\n",
    "    print(f\"PDF already exists at {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: AUTO (default)\n",
      "============================================================\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Time taken: 3.94 seconds\n",
      "Elements extracted: 276\n",
      "Element Distribution:\n",
      "------------------------------\n",
      "  Footer: 10\n",
      "  ListItem: 4\n",
      "  NarrativeText: 75\n",
      "  Text: 86\n",
      "  Title: 101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Text': 86, 'Title': 101, 'NarrativeText': 75, 'Footer': 10, 'ListItem': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Strategy 1: AUTO (Default)\n",
    "# Automatically selects the best strategy based on the document\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"Strategy: AUTO (default)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "elements_auto = partition(\n",
    "    filename=pdf_path,\n",
    "    strategy=\"auto\",  # This is the default\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Elements extracted: {len(elements_auto)}\")\n",
    "\n",
    "# Show element distribution\n",
    "analyze_elements(elements_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: FAST\n",
      "============================================================\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Time taken: 0.85 seconds\n",
      "Elements extracted: 276\n",
      "Element Distribution:\n",
      "------------------------------\n",
      "  Footer: 10\n",
      "  ListItem: 4\n",
      "  NarrativeText: 75\n",
      "  Text: 86\n",
      "  Title: 101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Text': 86, 'Title': 101, 'NarrativeText': 75, 'Footer': 10, 'ListItem': 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Strategy 2: FAST\n",
    "# Quick processing without using ML models\n",
    "# Good for simple documents or when speed is critical\n",
    "\n",
    "print(\"Strategy: FAST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "elements_fast = partition(\n",
    "    filename=pdf_path,\n",
    "    strategy=\"fast\",  # No ML models, faster processing\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Elements extracted: {len(elements_fast)}\")\n",
    "\n",
    "# Show element distribution\n",
    "analyze_elements(elements_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: HI_RES\n",
      "============================================================\n",
      "Note: hi_res strategy uses ML models and may take longer\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 63.62 seconds\n",
      "Elements extracted: 327\n",
      "Element Distribution:\n",
      "------------------------------\n",
      "  FigureCaption: 2\n",
      "  Footer: 2\n",
      "  Header: 4\n",
      "  Image: 20\n",
      "  ListItem: 20\n",
      "  NarrativeText: 58\n",
      "  Table: 4\n",
      "  Text: 192\n",
      "  Title: 25\n"
     ]
    }
   ],
   "source": [
    "# Strategy 3: HI_RES\n",
    "# High-resolution processing with ML models\n",
    "# Best for complex documents with tables, figures, and mixed layouts\n",
    "# Note: This requires additional dependencies and takes longer\n",
    "\n",
    "print(\"Strategy: HI_RES\")\n",
    "print(\"=\"*60)\n",
    "print(\"Note: hi_res strategy uses ML models and may take longer\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    elements_hires = partition_pdf(\n",
    "        filename=pdf_path,\n",
    "        strategy=\"hi_res\",\n",
    "        infer_table_structure=True,  # Extract table structure\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Elements extracted: {len(elements_hires)}\")\n",
    "    \n",
    "    # Show element distribution\n",
    "    analyze_elements(elements_hires)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"hi_res strategy requires additional dependencies.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nTo enable hi_res, install: pip install 'unstructured[pdf-infer]'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "First 5 elements from AUTO strategy:\n",
      "============================================================\n",
      "\n",
      "[0] Text:\n",
      "    4 2 0 2 c e D 9\n",
      "\n",
      "[1] Title:\n",
      "    ] L C . s c [\n",
      "\n",
      "[2] Text:\n",
      "    5 v 9 6 8 9 0 . 8 0 4 2 : v i X r a\n",
      "\n",
      "[3] Title:\n",
      "    Docling Technical Report\n",
      "\n",
      "[4] Title:\n",
      "    Version 1.0\n",
      "\n",
      "============================================================\n",
      "First 5 elements from FAST strategy:\n",
      "============================================================\n",
      "\n",
      "[0] Text:\n",
      "    4 2 0 2 c e D 9\n",
      "\n",
      "[1] Title:\n",
      "    ] L C . s c [\n",
      "\n",
      "[2] Text:\n",
      "    5 v 9 6 8 9 0 . 8 0 4 2 : v i X r a\n",
      "\n",
      "[3] Title:\n",
      "    Docling Technical Report\n",
      "\n",
      "[4] Title:\n",
      "    Version 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compare extracted text between strategies\n",
    "# Let's look at the first few elements from each strategy\n",
    "\n",
    "def show_first_elements(elements, n=5, strategy_name=\"\"):\n",
    "    \"\"\"Display the first n elements from a partition result.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"First {n} elements from {strategy_name} strategy:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, elem in enumerate(elements[:n]):\n",
    "        elem_type = type(elem).__name__\n",
    "        text = elem.text[:150] + \"...\" if len(elem.text) > 150 else elem.text\n",
    "        print(f\"\\n[{i}] {elem_type}:\")\n",
    "        print(f\"    {text}\")\n",
    "\n",
    "# Compare AUTO and FAST strategies\n",
    "show_first_elements(elements_auto, n=5, strategy_name=\"AUTO\")\n",
    "show_first_elements(elements_fast, n=5, strategy_name=\"FAST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Supported File Formats\n",
    "\n",
    "Unstructured supports 28+ file formats. Let's explore examples for the most common ones.\n",
    "\n",
    "### 4.1 PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "PDF Elements: 276\n",
      "\n",
      "First 3 elements:\n",
      "  - Text: 4 2 0 2 c e D 9...\n",
      "  - Title: ] L C . s c [...\n",
      "  - Text: 5 v 9 6 8 9 0 . 8 0 4 2 : v i X r a...\n"
     ]
    }
   ],
   "source": [
    "# PDF Partitioning with various options\n",
    "# Using partition_pdf for more control over PDF-specific settings\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Basic PDF partitioning\n",
    "pdf_elements = partition_pdf(\n",
    "    filename=pdf_path,\n",
    "    strategy=\"fast\",  # Use fast for this demo\n",
    ")\n",
    "\n",
    "print(f\"PDF Elements: {len(pdf_elements)}\")\n",
    "print(\"\\nFirst 3 elements:\")\n",
    "for elem in pdf_elements[:3]:\n",
    "    print(f\"  - {type(elem).__name__}: {elem.text[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Found 0 tables in the PDF\n"
     ]
    }
   ],
   "source": [
    "# PDF with table extraction enabled\n",
    "# This extracts structured table data from PDFs\n",
    "\n",
    "pdf_elements_tables = partition_pdf(\n",
    "    filename=pdf_path,\n",
    "    strategy=\"fast\",\n",
    "    include_page_breaks=True,  # Mark page boundaries\n",
    ")\n",
    "\n",
    "# Find tables in the document\n",
    "tables = [e for e in pdf_elements_tables if isinstance(e, Table)]\n",
    "print(f\"Found {len(tables)} tables in the PDF\")\n",
    "\n",
    "# Show first table if found\n",
    "if tables:\n",
    "    print(\"\\nFirst table content:\")\n",
    "    print(tables[0].text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document has 10 pages\n",
      "\n",
      "Page 0: 9 elements\n",
      "\n",
      "Page 1: 15 elements\n",
      "\n",
      "Page 2: 19 elements\n"
     ]
    }
   ],
   "source": [
    "# Get elements with page numbers\n",
    "# Page metadata is useful for citations and navigation\n",
    "\n",
    "# Group elements by page\n",
    "pages = {}\n",
    "for elem in pdf_elements_tables:\n",
    "    page_num = elem.metadata.page_number if elem.metadata.page_number else 0\n",
    "    if page_num not in pages:\n",
    "        pages[page_num] = []\n",
    "    pages[page_num].append(elem)\n",
    "\n",
    "print(f\"Document has {len(pages)} pages\")\n",
    "for page_num in sorted(pages.keys())[:3]:  # Show first 3 pages\n",
    "    print(f\"\\nPage {page_num}: {len(pages[page_num])} elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 HTML Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Elements: 20\n",
      "Element Distribution:\n",
      "------------------------------\n",
      "  ListItem: 8\n",
      "  NarrativeText: 6\n",
      "  Table: 1\n",
      "  Title: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Title': 5, 'NarrativeText': 6, 'ListItem': 8, 'Table': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HTML Partitioning\n",
    "# Unstructured can parse both local HTML files and URLs\n",
    "\n",
    "from unstructured.partition.html import partition_html\n",
    "\n",
    "# Partition the sample HTML file\n",
    "html_elements = partition_html(filename=\"sample_documents/sample.html\")\n",
    "\n",
    "print(f\"HTML Elements: {len(html_elements)}\")\n",
    "analyze_elements(html_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements from URL: 54\n",
      "  - Title: Documentation...\n",
      "  - Image: Docling...\n",
      "  - Image: DS4SD%2Fdocling | Trendshift...\n",
      "  - Image: arXiv...\n",
      "  - Image: PyPI version...\n"
     ]
    }
   ],
   "source": [
    "# HTML from URL (web scraping)\n",
    "# You can also partition HTML directly from a URL\n",
    "\n",
    "try:\n",
    "    url_elements = partition_html(\n",
    "        url=\"https://docling-project.github.io/docling/\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Elements from URL: {len(url_elements)}\")\n",
    "    for elem in url_elements[:5]:\n",
    "        print(f\"  - {type(elem).__name__}: {elem.text[:60]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch URL: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements from string: 6\n",
      "  - Title: Sample Document\n",
      "  - NarrativeText: This is a paragraph with some bold text.\n",
      "  - ListItem: Item 1\n",
      "  - ListItem: Item 2\n",
      "  - ListItem: Item 3\n",
      "  - Table: Name Value A 100 B 200\n"
     ]
    }
   ],
   "source": [
    "# HTML from string content\n",
    "# Useful when you have HTML content in memory\n",
    "\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <h1>Sample Document</h1>\n",
    "    <p>This is a paragraph with some <strong>bold text</strong>.</p>\n",
    "    <ul>\n",
    "        <li>Item 1</li>\n",
    "        <li>Item 2</li>\n",
    "        <li>Item 3</li>\n",
    "    </ul>\n",
    "    <table>\n",
    "        <tr><th>Name</th><th>Value</th></tr>\n",
    "        <tr><td>A</td><td>100</td></tr>\n",
    "        <tr><td>B</td><td>200</td></tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "elements_from_string = partition_html(text=html_content)\n",
    "\n",
    "print(f\"Elements from string: {len(elements_from_string)}\")\n",
    "for elem in elements_from_string:\n",
    "    print(f\"  - {type(elem).__name__}: {elem.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Markdown Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown Elements: 44\n",
      "Element Distribution:\n",
      "------------------------------\n",
      "  ListItem: 15\n",
      "  NarrativeText: 12\n",
      "  Table: 2\n",
      "  Text: 1\n",
      "  Title: 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Title': 14, 'NarrativeText': 12, 'ListItem': 15, 'Table': 2, 'Text': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Markdown Partitioning\n",
    "# Parses markdown syntax and extracts structured elements\n",
    "\n",
    "from unstructured.partition.md import partition_md\n",
    "\n",
    "# Partition the sample markdown file\n",
    "md_elements = partition_md(filename=\"sample_documents/sample.md\")\n",
    "\n",
    "print(f\"Markdown Elements: {len(md_elements)}\")\n",
    "analyze_elements(md_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown Structure:\n",
      "============================================================\n",
      "Title           | Document Parsing Best Practices\n",
      "NarrativeText   | A comprehensive guide to document parsing for RAG systems.\n",
      "Title           | Table of Contents\n",
      "ListItem        | Introduction\n",
      "ListItem        | Supported Formats\n",
      "ListItem        | Parsing Strategies\n",
      "ListItem        | Integration Guide\n",
      "Title           | Introduction\n",
      "NarrativeText   | Document parsing is a critical component in modern AI applications. It enables t...\n",
      "ListItem        | Build searchable knowledge bases\n",
      "ListItem        | Create training datasets for machine learning\n",
      "ListItem        | Enable semantic search and retrieval\n",
      "ListItem        | Power question-answering systems\n",
      "NarrativeText   | Note: The quality of document parsing directly impacts the performance of downst...\n",
      "Title           | Supported Formats\n"
     ]
    }
   ],
   "source": [
    "# Show markdown structure\n",
    "# Headings become Title elements, paragraphs become NarrativeText\n",
    "\n",
    "print(\"Markdown Structure:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for elem in md_elements[:15]:\n",
    "    elem_type = type(elem).__name__\n",
    "    text = elem.text[:80] + \"...\" if len(elem.text) > 80 else elem.text\n",
    "    print(f\"{elem_type:15} | {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements from markdown string: 7\n",
      "  - Title: Main Title\n",
      "  - NarrativeText: This is an introduction paragraph.\n",
      "  - Title: Section 1\n",
      "  - NarrativeText: Here's some content with: - Bullet point 1 - Bullet point 2\n",
      "  - Title: Section 2\n",
      "  - Text: def hello():\n",
      "    print(\"Hello, World!\")\n",
      "  - Table: Column A Column B Value 1 Value 2\n"
     ]
    }
   ],
   "source": [
    "# Markdown from string\n",
    "# Useful for processing markdown content in memory\n",
    "\n",
    "md_string = \"\"\"\n",
    "# Main Title\n",
    "\n",
    "This is an introduction paragraph.\n",
    "\n",
    "## Section 1\n",
    "\n",
    "Here's some content with:\n",
    "- Bullet point 1\n",
    "- Bullet point 2\n",
    "\n",
    "## Section 2\n",
    "\n",
    "```python\n",
    "def hello():\n",
    "    print(\"Hello, World!\")\n",
    "```\n",
    "\n",
    "| Column A | Column B |\n",
    "|----------|----------|\n",
    "| Value 1  | Value 2  |\n",
    "\"\"\"\n",
    "\n",
    "md_string_elements = partition_md(text=md_string)\n",
    "\n",
    "print(f\"Elements from markdown string: {len(md_string_elements)}\")\n",
    "for elem in md_string_elements:\n",
    "    print(f\"  - {type(elem).__name__}: {elem.text[:60]}...\" if len(elem.text) > 60 else f\"  - {type(elem).__name__}: {elem.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Microsoft Office Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample DOCX at: sample_documents/sample1.docx\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DOCX file for demonstration\n",
    "# This requires python-docx library\n",
    "\n",
    "try:\n",
    "    from docx import Document as DocxDocument\n",
    "    from docx.shared import Inches\n",
    "    \n",
    "    # Create a new document\n",
    "    doc = DocxDocument()\n",
    "    \n",
    "    # Add content\n",
    "    doc.add_heading('Sample Word Document', 0)\n",
    "    doc.add_paragraph('This is a sample Word document created for testing Unstructured.')\n",
    "    \n",
    "    doc.add_heading('Section 1: Introduction', level=1)\n",
    "    doc.add_paragraph('Unstructured is a powerful library for document parsing.')\n",
    "    \n",
    "    doc.add_heading('Section 2: Features', level=1)\n",
    "    doc.add_paragraph('Key features include:')\n",
    "    \n",
    "    # Add a bulleted list\n",
    "    doc.add_paragraph('Multiple file format support', style='List Bullet')\n",
    "    doc.add_paragraph('OCR capabilities', style='List Bullet')\n",
    "    doc.add_paragraph('Table extraction', style='List Bullet')\n",
    "    \n",
    "    # Add a simple table\n",
    "    table = doc.add_table(rows=3, cols=2)\n",
    "    table.style = 'Table Grid'\n",
    "    cells = table.rows[0].cells\n",
    "    cells[0].text = 'Feature'\n",
    "    cells[1].text = 'Status'\n",
    "    cells = table.rows[1].cells\n",
    "    cells[0].text = 'PDF Support'\n",
    "    cells[1].text = 'Available'\n",
    "    cells = table.rows[2].cells\n",
    "    cells[0].text = 'OCR'\n",
    "    cells[1].text = 'Available'\n",
    "    \n",
    "    # Save the document\n",
    "    docx_path = 'sample_documents/sample1.docx'\n",
    "    doc.save(docx_path)\n",
    "    print(f\"Created sample DOCX at: {docx_path}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"python-docx not installed. Install with: pip install python-docx\")\n",
    "    docx_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCX Elements: 10\n",
      "Element Distribution:\n",
      "------------------------------\n",
      "  ListItem: 3\n",
      "  NarrativeText: 3\n",
      "  Table: 1\n",
      "  Title: 3\n",
      "\n",
      "Content:\n",
      "  - Title: Sample Word Document\n",
      "  - NarrativeText: This is a sample Word document created for testing Unstructured.\n",
      "  - Title: Section 1: Introduction\n",
      "  - NarrativeText: Unstructured is a powerful library for document parsing.\n",
      "  - Title: Section 2: Features\n",
      "  - NarrativeText: Key features include:\n",
      "  - ListItem: Multiple file format support\n",
      "  - ListItem: OCR capabilities\n",
      "  - ListItem: Table extraction\n",
      "  - Table: Feature Status PDF Support Available OCR Available\n"
     ]
    }
   ],
   "source": [
    "# DOCX Partitioning\n",
    "# Parse Microsoft Word documents\n",
    "\n",
    "from unstructured.partition.docx import partition_docx\n",
    "\n",
    "if docx_path and os.path.exists(docx_path):\n",
    "    docx_elements = partition_docx(filename=docx_path)\n",
    "    \n",
    "    print(f\"DOCX Elements: {len(docx_elements)}\")\n",
    "    analyze_elements(docx_elements)\n",
    "    \n",
    "    print(\"\\nContent:\")\n",
    "    for elem in docx_elements:\n",
    "        print(f\"  - {type(elem).__name__}: {elem.text}\")\n",
    "else:\n",
    "    print(\"No DOCX file available for demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample XLSX at: sample_documents/sample1.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Create a sample XLSX file for demonstration\n",
    "\n",
    "try:\n",
    "    import openpyxl\n",
    "    \n",
    "    # Create a new workbook\n",
    "    wb = openpyxl.Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Data\"\n",
    "    \n",
    "    # Add headers\n",
    "    ws['A1'] = 'Product'\n",
    "    ws['B1'] = 'Category'\n",
    "    ws['C1'] = 'Price'\n",
    "    ws['D1'] = 'Quantity'\n",
    "    \n",
    "    # Add data\n",
    "    data = [\n",
    "        ('Laptop', 'Electronics', 999.99, 50),\n",
    "        ('Phone', 'Electronics', 599.99, 100),\n",
    "        ('Desk', 'Furniture', 299.99, 25),\n",
    "        ('Chair', 'Furniture', 149.99, 75),\n",
    "        ('Monitor', 'Electronics', 349.99, 40),\n",
    "    ]\n",
    "    \n",
    "    for row_idx, row_data in enumerate(data, start=2):\n",
    "        for col_idx, value in enumerate(row_data, start=1):\n",
    "            ws.cell(row=row_idx, column=col_idx, value=value)\n",
    "    \n",
    "    # Save the workbook\n",
    "    xlsx_path = 'sample_documents/sample1.xlsx'\n",
    "    wb.save(xlsx_path)\n",
    "    print(f\"Created sample XLSX at: {xlsx_path}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"openpyxl not installed. Install with: pip install openpyxl\")\n",
    "    xlsx_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLSX Elements: 11\n",
      "Element Distribution:\n",
      "------------------------------\n",
      "  NarrativeText: 4\n",
      "  Table: 3\n",
      "  Text: 2\n",
      "  Title: 2\n",
      "\n",
      "Content:\n",
      "  - Table: Dates Modules 2 November 2025 9 November 2025 16 November 2025 23 November 2025 30 November 2025 7 December 2025 Module 4: Document Parsers + Module 5 (Part 1): LlamaIndex Fundamentals 14 December 202...\n",
      "  - Text: 12 April 2026\n",
      "  - Title: Black Friday 2025: AI Services & Tools Discount Guide\n",
      "  - Table: by Seulki Kang https://www.linkedin.com/in/seulki-kang/\n",
      "  - Title: Crawled by Genspark AI Sheet\n",
      "  - NarrativeText: Want to try Genspark yourself?\n",
      "  - NarrativeText: ðŸ”— Get 1,000 free credits: https://www.genspark.ai/invite_member?invite_code=NTQxMmI5M2ZMOTRhMUwyODg2TDhjZmZMNzE3ODRhYmI0Y2Jl\n",
      "  - NarrativeText: ðŸ”— 10% discount code: https://www.genspark.ai/?via=seulki875 (US) / https://www.genspark.ai/?via=seulki (Other Countries)\n",
      "  - Table: Name Description URL Benefits Monthly Plan Annual Plan Notes â­ Popular AI Services (Discount Info) Perplexity Pro AI search engine, real-time information search https://perplexity.ai 50% cashback with...\n",
      "  - Text: âœ… Total 39 AI Services (Discounts Available) | Discount Range 20%~85% | Black Friday 2025 (11/28)\n",
      "  - NarrativeText: ðŸ“ Fact-Checked: All discount information verified from official sources as of November 28, 2025\n"
     ]
    }
   ],
   "source": [
    "# XLSX Partitioning\n",
    "# Parse Microsoft Excel spreadsheets\n",
    "\n",
    "from unstructured.partition.xlsx import partition_xlsx\n",
    "\n",
    "xlsx_path = \"sample_documents/sample.xlsx\"\n",
    "if xlsx_path and os.path.exists(xlsx_path):\n",
    "    xlsx_elements = partition_xlsx(filename=xlsx_path)\n",
    "    \n",
    "    print(f\"XLSX Elements: {len(xlsx_elements)}\")\n",
    "    analyze_elements(xlsx_elements)\n",
    "    \n",
    "    print(\"\\nContent:\")\n",
    "    for elem in xlsx_elements:\n",
    "        text = elem.text[:200] + \"...\" if len(elem.text) > 200 else elem.text\n",
    "        print(f\"  - {type(elem).__name__}: {text}\")\n",
    "else:\n",
    "    print(\"No XLSX file available for demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample PPTX at: sample_documents/sample1.pptx\n"
     ]
    }
   ],
   "source": [
    "# Create a sample PPTX file for demonstration\n",
    "\n",
    "try:\n",
    "    from pptx import Presentation\n",
    "    from pptx.util import Inches, Pt\n",
    "    \n",
    "    # Create a presentation\n",
    "    prs = Presentation()\n",
    "    \n",
    "    # Add title slide\n",
    "    title_slide_layout = prs.slide_layouts[0]\n",
    "    slide = prs.slides.add_slide(title_slide_layout)\n",
    "    title = slide.shapes.title\n",
    "    subtitle = slide.placeholders[1]\n",
    "    title.text = \"Unstructured Demo\"\n",
    "    subtitle.text = \"Document Parsing for RAG\"\n",
    "    \n",
    "    # Add content slide\n",
    "    bullet_slide_layout = prs.slide_layouts[1]\n",
    "    slide = prs.slides.add_slide(bullet_slide_layout)\n",
    "    shapes = slide.shapes\n",
    "    title_shape = shapes.title\n",
    "    body_shape = shapes.placeholders[1]\n",
    "    title_shape.text = \"Key Features\"\n",
    "    tf = body_shape.text_frame\n",
    "    tf.text = \"Multiple file formats supported\"\n",
    "    p = tf.add_paragraph()\n",
    "    p.text = \"OCR for scanned documents\"\n",
    "    p.level = 0\n",
    "    p = tf.add_paragraph()\n",
    "    p.text = \"Table extraction\"\n",
    "    p.level = 0\n",
    "    \n",
    "    # Save the presentation\n",
    "    pptx_path = 'sample_documents/sample1.pptx'\n",
    "    prs.save(pptx_path)\n",
    "    print(f\"Created sample PPTX at: {pptx_path}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"python-pptx not installed. Install with: pip install python-pptx\")\n",
    "    pptx_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPTX Elements: 23\n",
      "Element Distribution:\n",
      "------------------------------\n",
      "  NarrativeText: 2\n",
      "  PageBreak: 14\n",
      "  Text: 3\n",
      "  Title: 4\n",
      "\n",
      "Content:\n",
      "  - Title: Stop searching. Start building\n",
      "  - Title: https://docrag.evolvue.ai/\n",
      "  - PageBreak: \n",
      "  - PageBreak: \n",
      "  - PageBreak: \n",
      "  - PageBreak: \n",
      "  - PageBreak: \n",
      "  - PageBreak: \n",
      "  - Title: Universal Compatibility\n",
      "  - NarrativeText: Seamlessly integrate with the tools you already use and love (with more to come soon!)\n",
      "  - PageBreak: \n",
      "  - Title: Multi-Model Intelligence\n",
      "  - NarrativeText: Choose from the best AI models in the industry to get accurate and contextual responses for every query\n",
      "  - PageBreak: \n",
      "  - PageBreak: \n",
      "  - PageBreak: \n",
      "  - PageBreak: \n",
      "  - PageBreak: \n",
      "  - Text: 1.28M+\n",
      "  - Text: 281k+\n",
      "  - Text: 361k+\n",
      "  - PageBreak: \n",
      "  - PageBreak: \n"
     ]
    }
   ],
   "source": [
    "# PPTX Partitioning\n",
    "# Parse Microsoft PowerPoint presentations\n",
    "\n",
    "from unstructured.partition.pptx import partition_pptx\n",
    "\n",
    "pptx_path = \"sample_documents/sample.pptx\"\n",
    "if pptx_path and os.path.exists(pptx_path):\n",
    "    pptx_elements = partition_pptx(filename=pptx_path)\n",
    "    \n",
    "    print(f\"PPTX Elements: {len(pptx_elements)}\")\n",
    "    analyze_elements(pptx_elements)\n",
    "    \n",
    "    print(\"\\nContent:\")\n",
    "    for elem in pptx_elements:\n",
    "        print(f\"  - {type(elem).__name__}: {elem.text}\")\n",
    "else:\n",
    "    print(\"No PPTX file available for demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Image Files (with OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample image at: sample_documents/sample_text_image.png\n"
     ]
    }
   ],
   "source": [
    "# Create a sample image with text for OCR demo\n",
    "\n",
    "try:\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "    \n",
    "    # Create a white image\n",
    "    img = Image.new('RGB', (800, 400), color='white')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Try to use a basic font, fall back to default if not available\n",
    "    try:\n",
    "        font_title = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 36)\n",
    "        font_text = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
    "    except:\n",
    "        font_title = ImageFont.load_default()\n",
    "        font_text = ImageFont.load_default()\n",
    "    \n",
    "    # Add text to the image\n",
    "    draw.text((50, 30), \"Document Parsing Demo\", fill='black', font=font_title)\n",
    "    draw.text((50, 100), \"This is sample text for OCR testing.\", fill='black', font=font_text)\n",
    "    draw.text((50, 150), \"Unstructured can extract text from images.\", fill='black', font=font_text)\n",
    "    draw.text((50, 200), \"Features:\", fill='black', font=font_text)\n",
    "    draw.text((70, 250), \"â€¢ Multiple language support\", fill='black', font=font_text)\n",
    "    draw.text((70, 300), \"â€¢ Various OCR backends\", fill='black', font=font_text)\n",
    "    draw.text((70, 350), \"â€¢ High accuracy extraction\", fill='black', font=font_text)\n",
    "    \n",
    "    # Save the image\n",
    "    image_path = 'sample_documents/sample_text_image.png'\n",
    "    img.save(image_path)\n",
    "    print(f\"Created sample image at: {image_path}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Pillow not installed. Install with: pip install Pillow\")\n",
    "    image_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Image Elements: 7\n",
      "Element Distribution:\n",
      "------------------------------\n",
      "  Footer: 1\n",
      "  Header: 1\n",
      "  NarrativeText: 5\n",
      "\n",
      "Extracted Text:\n",
      "  - Header: Document Parsing Demo\n",
      "  - NarrativeText: This issample text for OCR testing.\n",
      "  - NarrativeText: Unstructured can extract text from images.\n",
      "  - NarrativeText: Features\n",
      "  - NarrativeText: 8 Multiple language support\n",
      "  - NarrativeText: B VariousOCR backends\n",
      "  - Footer: 8 High accuracy extraction\n"
     ]
    }
   ],
   "source": [
    "# Image Partitioning with OCR\n",
    "# Extract text from images using OCR\n",
    "\n",
    "from unstructured.partition.image import partition_image\n",
    "\n",
    "if image_path and os.path.exists(image_path):\n",
    "    try:\n",
    "        # Partition the image using OCR\n",
    "        image_elements = partition_image(\n",
    "            filename=image_path,\n",
    "            strategy=\"auto\",  # Will use OCR automatically\n",
    "        )\n",
    "        \n",
    "        print(f\"Image Elements: {len(image_elements)}\")\n",
    "        analyze_elements(image_elements)\n",
    "        \n",
    "        print(\"\\nExtracted Text:\")\n",
    "        for elem in image_elements:\n",
    "            print(f\"  - {type(elem).__name__}: {elem.text}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"OCR error: {e}\")\n",
    "        print(\"\\nOCR requires additional dependencies.\")\n",
    "        print(\"Install with: pip install 'unstructured[local-inference]'\")\n",
    "        print(\"Also ensure Tesseract OCR is installed on your system.\")\n",
    "else:\n",
    "    print(\"No image file available for demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Element Types Deep Dive\n",
    "\n",
    "Let's explore the different element types and their properties in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Elements Analysis\n",
      "============================================================\n",
      "Total titles found: 14\n",
      "\n",
      "Title 1:\n",
      "  Text: Document Parsing Best Practices\n",
      "  ID: 9ea1b6ba00245b0da1c77098a9e130d5\n",
      "  Category Depth: 0\n",
      "\n",
      "Title 2:\n",
      "  Text: Table of Contents\n",
      "  ID: bf0aeb6089cc5feb43bf6f5734f3d849\n",
      "  Category Depth: 1\n",
      "\n",
      "Title 3:\n",
      "  Text: Introduction\n",
      "  ID: be39b6913500d3c48b592988ca400449\n",
      "  Category Depth: 1\n",
      "\n",
      "Title 4:\n",
      "  Text: Supported Formats\n",
      "  ID: bfe92481356969a08531b7d9e04d44ac\n",
      "  Category Depth: 1\n",
      "\n",
      "Title 5:\n",
      "  Text: Primary Formats\n",
      "  ID: e88f38a1f4f2fab33f199c2dd9224d98\n",
      "  Category Depth: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed look at Title elements\n",
    "# Titles represent headings and section headers\n",
    "\n",
    "# Get titles from the markdown document\n",
    "titles = [e for e in md_elements if isinstance(e, Title)]\n",
    "\n",
    "print(\"Title Elements Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total titles found: {len(titles)}\")\n",
    "print()\n",
    "\n",
    "for i, title in enumerate(titles[:5]):\n",
    "    print(f\"Title {i+1}:\")\n",
    "    print(f\"  Text: {title.text}\")\n",
    "    print(f\"  ID: {title.id}\")\n",
    "    \n",
    "    # Check for category depth (heading level)\n",
    "    if hasattr(title.metadata, 'category_depth'):\n",
    "        print(f\"  Category Depth: {title.metadata.category_depth}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NarrativeText Elements Analysis\n",
      "============================================================\n",
      "Total narrative texts: 12\n",
      "\n",
      "Total characters: 1227\n",
      "Average characters per element: 102.2\n",
      "\n",
      "Sample narrative texts:\n",
      "  - A comprehensive guide to document parsing for RAG systems.\n",
      "  - Document parsing is a critical component in modern AI applications. It enables the extraction of str...\n",
      "  - Note: The quality of document parsing directly impacts the performance of downstream AI applications...\n"
     ]
    }
   ],
   "source": [
    "# NarrativeText elements - main body content\n",
    "\n",
    "narrative_texts = [e for e in md_elements if isinstance(e, NarrativeText)]\n",
    "\n",
    "print(\"NarrativeText Elements Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total narrative texts: {len(narrative_texts)}\")\n",
    "print()\n",
    "\n",
    "# Calculate text statistics\n",
    "total_chars = sum(len(n.text) for n in narrative_texts)\n",
    "avg_chars = total_chars / len(narrative_texts) if narrative_texts else 0\n",
    "\n",
    "print(f\"Total characters: {total_chars}\")\n",
    "print(f\"Average characters per element: {avg_chars:.1f}\")\n",
    "print()\n",
    "\n",
    "print(\"Sample narrative texts:\")\n",
    "for text in narrative_texts[:3]:\n",
    "    preview = text.text[:100] + \"...\" if len(text.text) > 100 else text.text\n",
    "    print(f\"  - {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Elements Analysis\n",
      "============================================================\n",
      "Total tables: 1\n",
      "\n",
      "Table 1:\n",
      "  Content: Feature Description Use Case OCR Support Optical Character Recognition for scanned documents Scanned PDFs, Images Table Extraction Structured table data extraction Financial reports, Data tables Layout Analysis Understanding document structure Academic papers, Legal documents Image Processing Extrac...\n",
      "  HTML available: Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Table elements - structured tabular data\n",
    "\n",
    "# Get tables from HTML (which has a table)\n",
    "tables = [e for e in html_elements if isinstance(e, Table)]\n",
    "\n",
    "print(\"Table Elements Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total tables: {len(tables)}\")\n",
    "print()\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"Table {i+1}:\")\n",
    "    print(f\"  Content: {table.text[:300]}...\" if len(table.text) > 300 else f\"  Content: {table.text}\")\n",
    "    \n",
    "    # Check for HTML representation\n",
    "    if hasattr(table.metadata, 'text_as_html') and table.metadata.text_as_html:\n",
    "        print(f\"  HTML available: Yes\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListItem Elements Analysis\n",
      "============================================================\n",
      "Total list items: 8\n",
      "\n",
      "List items found:\n",
      "  â€¢ Automated data extraction\n",
      "  â€¢ Structured content analysis\n",
      "  â€¢ Integration with AI/ML pipelines\n",
      "  â€¢ Support for multiple formats\n",
      "  â€¢ Knowledge Base Creation: Convert documents into searchable chunks\n",
      "  â€¢ Semantic Search: Enable meaning-based document retrieval\n",
      "  â€¢ Question Answering: Provide accurate answers from document context\n",
      "  â€¢ Document Summarization: Generate concise summaries\n"
     ]
    }
   ],
   "source": [
    "# ListItem elements - bullet points and numbered items\n",
    "\n",
    "list_items = [e for e in html_elements if isinstance(e, ListItem)]\n",
    "\n",
    "print(\"ListItem Elements Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total list items: {len(list_items)}\")\n",
    "print()\n",
    "\n",
    "print(\"List items found:\")\n",
    "for item in list_items:\n",
    "    print(f\"  â€¢ {item.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element as Dictionary:\n",
      "{\n",
      "  \"type\": \"Title\",\n",
      "  \"element_id\": \"2a000fb58dc445e313c5dffe0ef3a614\",\n",
      "  \"text\": \"1. Overview\",\n",
      "  \"metadata\": {\n",
      "    \"category_depth\": 1,\n",
      "    \"last_modified\": \"2025-11-28T09:19:12\",\n",
      "    \"languages\": [\n",
      "      \"eng\"\n",
      "    ],\n",
      "    \"file_directory\": \"sample_documents\",\n",
      "    \"filename\": \"sample.html\",\n",
      "    \"filetype\": \"text/html\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Converting elements to different formats\n",
    "\n",
    "# Convert to dictionary\n",
    "element_dict = html_elements[0].to_dict()\n",
    "print(\"Element as Dictionary:\")\n",
    "print(json.dumps(element_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 elements as JSON:\n",
      "[\n",
      "    {\n",
      "        \"element_id\": \"2a000fb58dc445e313c5dffe0ef3a614\",\n",
      "        \"metadata\": {\n",
      "            \"category_depth\": 1,\n",
      "            \"file_directory\": \"sample_documents\",\n",
      "            \"filename\": \"sample.html\",\n",
      "            \"filetype\": \"text/html\",\n",
      "            \"languages\": [\n",
      "                \"eng\"\n",
      "            ],\n",
      "            \"last_modified\": \"2025-11-28T09:19:12\"\n",
      "        },\n",
      "        \"text\": \"1. Overview\",\n",
      "        \"type\": \"Title\"\n",
      "    },\n",
      "    {\n",
      "        \"element_id\": \"0111c3838e668907531ba2247a8d97b1\",\n",
      "        \"metadata\": {\n",
      "            \"file_directory\": \"sample_documents\",\n",
      "            \"filename\": \"sample.html\",\n",
      "            \"filetype\": \"text/html\",\n",
      "            \"languages\": [\n",
      "                \"eng\"\n",
      "            ],\n",
      "            \"last_modified\": \"2025-11-28T09:19:12\",\n",
      "            \"parent_id\": \"2a000fb58dc445e313c5dffe0ef3a614\"\n",
      "        },\n",
      "        \"text\": \"Document parsing is the process of analyzing and extracting structured information from various document formats. This includes PDFs, Word documents,...\n"
     ]
    }
   ],
   "source": [
    "# Convert all elements to a list of dictionaries\n",
    "# Useful for serialization and export\n",
    "\n",
    "from unstructured.staging.base import elements_to_json\n",
    "\n",
    "# Convert to JSON string\n",
    "json_output = elements_to_json(html_elements[:5])\n",
    "print(\"First 5 elements as JSON:\")\n",
    "print(json_output[:1000] + \"...\" if len(json_output) > 1000 else json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Working with Metadata\n",
    "\n",
    "Each element in Unstructured contains rich metadata that can be used for filtering, citations, and enhanced retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element Metadata Fields\n",
      "============================================================\n",
      "coordinates: {'points': ((16.34, 216.16000000000008), (16.34, 308.36), (36.34, 308.36), (36.34, 216.16000000000008)), 'system': 'PixelSpace', 'layout_width': 612.0, 'layout_height': 792.0}\n",
      "file_directory: sample_documents\n",
      "filename: docling_paper.pdf\n",
      "last_modified: 2025-12-04T13:44:43\n",
      "page_number: 1\n",
      "languages: ['eng']\n",
      "filetype: application/pdf\n"
     ]
    }
   ],
   "source": [
    "# Exploring element metadata\n",
    "\n",
    "# Get a sample element from PDF\n",
    "sample_element = pdf_elements[0]\n",
    "\n",
    "print(\"Element Metadata Fields\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get all metadata as dictionary\n",
    "metadata_dict = sample_element.metadata.to_dict()\n",
    "\n",
    "for key, value in metadata_dict.items():\n",
    "    if value is not None:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Metadata Fields:\n",
      "============================================================\n",
      "\n",
      "filename:\n",
      "  Source file name - useful for citations\n",
      "\n",
      "file_directory:\n",
      "  Directory path of the source file\n",
      "\n",
      "filetype:\n",
      "  MIME type of the source document\n",
      "\n",
      "page_number:\n",
      "  Page number in the document\n",
      "\n",
      "coordinates:\n",
      "  Bounding box coordinates (for layout analysis)\n",
      "\n",
      "text_as_html:\n",
      "  HTML representation (for tables)\n",
      "\n",
      "category_depth:\n",
      "  Heading level (for titles)\n",
      "\n",
      "languages:\n",
      "  Detected languages in the text\n",
      "\n",
      "emphasized_text_contents:\n",
      "  Bold/italic text\n",
      "\n",
      "link_urls:\n",
      "  URLs found in the element\n"
     ]
    }
   ],
   "source": [
    "# Common metadata fields and their uses\n",
    "\n",
    "print(\"Common Metadata Fields:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metadata_info = {\n",
    "    \"filename\": \"Source file name - useful for citations\",\n",
    "    \"file_directory\": \"Directory path of the source file\",\n",
    "    \"filetype\": \"MIME type of the source document\",\n",
    "    \"page_number\": \"Page number in the document\",\n",
    "    \"coordinates\": \"Bounding box coordinates (for layout analysis)\",\n",
    "    \"text_as_html\": \"HTML representation (for tables)\",\n",
    "    \"category_depth\": \"Heading level (for titles)\",\n",
    "    \"languages\": \"Detected languages in the text\",\n",
    "    \"emphasized_text_contents\": \"Bold/italic text\",\n",
    "    \"link_urls\": \"URLs found in the element\",\n",
    "}\n",
    "\n",
    "for field, description in metadata_info.items():\n",
    "    print(f\"\\n{field}:\")\n",
    "    print(f\"  {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements on page 1: 15\n",
      "\n",
      "First 5 elements:\n",
      "  - Text: 4 2 0 2 c e D 9\n",
      "  - Title: ] L C . s c [\n",
      "  - Text: 5 v 9 6 8 9 0 . 8 0 4 2 : v i X r a\n",
      "  - Title: Docling Technical Report\n",
      "  - Title: Version 1.0\n"
     ]
    }
   ],
   "source": [
    "# Filter elements by metadata\n",
    "# Example: Get elements from a specific page\n",
    "\n",
    "# Get all elements from page 1\n",
    "page_1_elements = [\n",
    "    e for e in pdf_elements \n",
    "    if e.metadata.page_number == 1\n",
    "]\n",
    "\n",
    "print(f\"Elements on page 1: {len(page_1_elements)}\")\n",
    "print(\"\\nFirst 5 elements:\")\n",
    "for elem in page_1_elements[:5]:\n",
    "    text = elem.text[:80] + \"...\" if len(elem.text) > 80 else elem.text\n",
    "    print(f\"  - {type(elem).__name__}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements with custom metadata:\n",
      "  - 1. Overview...\n",
      "    detection_origin: None\n",
      "  - Document parsing is the process of analyzing and e...\n",
      "    detection_origin: None\n",
      "  - 1.1 Key Benefits...\n",
      "    detection_origin: None\n"
     ]
    }
   ],
   "source": [
    "# Add custom metadata to elements\n",
    "# Useful for tracking processing information\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a copy of elements with custom metadata\n",
    "processed_elements = []\n",
    "for elem in html_elements[:3]:\n",
    "    # Access the metadata and add custom fields\n",
    "    elem.metadata.detection_origin = \"unstructured-notebook-demo\"\n",
    "    processed_elements.append(elem)\n",
    "\n",
    "# Verify custom metadata was added\n",
    "print(\"Elements with custom metadata:\")\n",
    "for elem in processed_elements:\n",
    "    print(f\"  - {elem.text[:50]}...\")\n",
    "    print(f\"    detection_origin: {elem.metadata.detection_origin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## OCR Configuration\n",
    "\n",
    "Unstructured supports multiple OCR backends for extracting text from scanned documents and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available OCR Backends:\n",
      "============================================================\n",
      "\n",
      "TESSERACT:\n",
      "  description: Google's Tesseract OCR - widely used, good accuracy\n",
      "  installation: apt-get install tesseract-ocr\n",
      "  languages: 100+ languages supported\n",
      "\n",
      "PADDLE:\n",
      "  description: PaddleOCR - excellent for multi-language documents\n",
      "  installation: pip install paddlepaddle paddleocr\n",
      "  languages: 80+ languages, strong CJK support\n"
     ]
    }
   ],
   "source": [
    "# OCR options overview\n",
    "\n",
    "ocr_options = {\n",
    "    \"tesseract\": {\n",
    "        \"description\": \"Google's Tesseract OCR - widely used, good accuracy\",\n",
    "        \"installation\": \"apt-get install tesseract-ocr\",\n",
    "        \"languages\": \"100+ languages supported\",\n",
    "    },\n",
    "    \"paddle\": {\n",
    "        \"description\": \"PaddleOCR - excellent for multi-language documents\",\n",
    "        \"installation\": \"pip install paddlepaddle paddleocr\",\n",
    "        \"languages\": \"80+ languages, strong CJK support\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Available OCR Backends:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for backend, info in ocr_options.items():\n",
    "    print(f\"\\n{backend.upper()}:\")\n",
    "    for key, value in info.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ocr_languages kwarg will be deprecated in a future version of unstructured. Please use languages instead.\n",
      "Only one of languages and ocr_languages should be specified. languages is preferred. ocr_languages is marked for deprecation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR Configuration Example:\n",
      "{\n",
      "  \"strategy\": \"hi_res\",\n",
      "  \"ocr_languages\": \"eng\",\n",
      "  \"hi_res_model_name\": \"yolox\"\n",
      "}\n",
      "\n",
      "OCR extracted 5 elements\n",
      "  - G@drant\n",
      "  - QDRANT VECTOR DATABASE: TYPICAL WORKFLOW & OPERATIONS IN PRACTICE\n",
      "  - SETUP INSERT DATA QUERY & REFINE MAINTAIN MONITOR & BACKUP (Foundation) (Pipeline) (Search & Filter) (Management) (Health & Safety) Sal le aN yd f=>y update_payload() ip I (External) (P=> \\t ) fe Modify metadata. \\ fal e7> Cn and Check collection ne wae 7 - > Update embeddings. | G J Retrieve points. a aes use filters 1 > View collection _ y leas | Remove points. COLLECTION 4 Tie mama upsert() => recommend() FILTER Find items based on positive/negative JUSS examples. with search() ee ; create_snapshot() Define collection parameters Generate vector embeddings Apply metadata filters to refine Create a point-in-time (name, dimension, metric) to and payloads from raw data, results (e.g., by category, date). backup for recovery. establish the vector space. then insert or update points.\n",
      "  - size.\n",
      "  - details.\n"
     ]
    }
   ],
   "source": [
    "# OCR configuration example with Tesseract\n",
    "# Note: Tesseract must be installed on your system\n",
    "\n",
    "# Example configuration for OCR\n",
    "ocr_config = {\n",
    "    \"strategy\": \"hi_res\",      # Enable high-resolution processing\n",
    "    \"ocr_languages\": \"eng\",    # Language(s) for OCR\n",
    "    \"hi_res_model_name\": \"yolox\",  # Layout detection model\n",
    "}\n",
    "\n",
    "print(\"OCR Configuration Example:\")\n",
    "print(json.dumps(ocr_config, indent=2))\n",
    "\n",
    "image_path = \"sample_documents/vdb.png\"\n",
    "# Partition with OCR (if tesseract is installed)\n",
    "if image_path and os.path.exists(image_path):\n",
    "    try:\n",
    "        ocr_elements = partition_image(\n",
    "            filename=image_path,\n",
    "            strategy=\"hi_res\",\n",
    "            ocr_languages=\"eng\",\n",
    "        )\n",
    "        print(f\"\\nOCR extracted {len(ocr_elements)} elements\")\n",
    "        for elem in ocr_elements:\n",
    "            print(f\"  - {elem.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nOCR not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ocr_languages kwarg will be deprecated in a future version of unstructured. Please use languages instead.\n",
      "Only one of languages and ocr_languages should be specified. languages is preferred. ocr_languages is marked for deprecation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forced OCR Configuration for Scanned PDFs:\n",
      "============================================================\n",
      "Elements from scanned PDF: 52\n",
      "  - Title: (wSins. Sinnt lousits\n",
      "  - NarrativeText: i bent Pasposs â€˜- Col cult the dis\n",
      "  - Title: CoSing Genâ€™ loseity Leah low Cosive Distonce Low Hieh\n",
      "  - Title: Jase st action Shi fs\n",
      "  - Title: bo vectouS\n",
      "  - Title: Fosemulor 7 SA, BA\n",
      "  - Title: A: B vec fang\n",
      "  - NarrativeText: wi multi lication, >) = Sum of slpmeal Se P\n",
      "  - Title: lngth. | Magaituds of the v\n",
      "  - Title: jie | > {ee\n",
      "  - Title: Podon A Puckon. B\n",
      "  - Title: lint] =\n",
      "  - NarrativeText: h, ete ra) Roti â€œ â€œ4 RomontÂ® Rating \\lictow 3\n",
      "  - Title: Parson A 5 ro mp 10 & Ty mine TTY\n",
      "  - Title: Step 4: Del Product A.B (sxto) + (ax)=68\n",
      "  - Title: Step A lr Cole the Moqnidudis\n",
      "  - NarrativeText: [Imi } = { ght Be _ fst x 583\n",
      "  - Text: | (5) | = n Joâ€ + a =\n",
      "  - Text: Apply the Josemula 68 Cosine Gina loxi ty = a3 KING x 7,\n",
      "  - NarrativeText: Band BR ok resvly â€˜ntical .\n",
      "  - Title: \\VVetose Visuel ae\n",
      "  - NarrativeText: Cosa Coho A (bo > a FO A 5 â€”? 7 B B Cosine = 1 Parone elas (...\n",
      "  - Title: Â» of Pys 0 duct\n",
      "  - Title: Stab Sle Apply Hae frvtmaucle a0 ad = â€”â€” = 06+\n",
      "  - Text: 583 x Bl Meta Puton A A Pra kon C- Ju di}fuint Los he\n",
      "  - Title: \") Fuclidion Distenee (tz)\n",
      "  - Title: Foyonualo: > | (a)- bi) 4 a, -b) te\n",
      "  - Title: !) See ight li Digtan CL\n",
      "  - Title: K Movhaton Nistonce (L1)\n",
      "  - Title: four\n",
      "  - Title: pus Vue\n",
      "  - Title: Wlosed > Vee\n",
      "  - NarrativeText: wowed wm budding tec gpa\n",
      "  - Title: Two Ashi tectueS\n",
      "  - Text: â€”â€”â€”â€”\n",
      "  - Title: (Bow 2) Ship ~ bom \\\n",
      "  - Title: lontinous Bog of WosrdS Corsabovey\n",
      "  - Title: INPUT PROJECTION â€” OUTPUT\n",
      "  - Title: opt wr\n",
      "  - Title: = fuuit | cBOwW â€˜ bungronding | contuxt wood\n",
      "  - Title: bast fui :\n",
      "  - NarrativeText: D) Vary tes to Lean 2) {ood wilh faeqgoet woudl.\n",
      "  - Title: foe woe woud\n",
      "  - NarrativeText: d 7 ee Topuct Â» bust bosad en bode \\. bavy woudS Boats of Si...\n",
      "  - NarrativeText: Con Clo wee fo tuain of\n",
      "  - Title: apooud 1, CROW\n",
      "  - Title: iosed 2 Vcr (2018)\n",
      "  - Title: 7 Che [low Nswseetl Nite oui\n",
      "  - NarrativeText: Hidden bo Taput me _ Frahedlng Pg) -Â» ud gare ( Veesbuloe Ss...\n",
      "  - Title: Contax] Window ee\n",
      "  - Title: LLM lige 10 Kanna. Cim ta\n",
      "  - Title: Cre 3\n"
     ]
    }
   ],
   "source": [
    "# PDF with forced OCR\n",
    "# Use ocr_only strategy when you know the PDF is scanned\n",
    "\n",
    "print(\"Forced OCR Configuration for Scanned PDFs:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example configuration\n",
    "#scanned_pdf_config =\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "elements = partition_pdf(\n",
    "    filename=\"sample_documents/scan.pdf\",\n",
    "    strategy=\"ocr_only\",      # Force OCR processing\n",
    "    ocr_languages=\"eng\",  # Multiple languages (English + French)\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Elements from scanned PDF: {len(elements)}\")\n",
    "\n",
    "for elem in elements:\n",
    "    print(f\"  - {type(elem).__name__}: {elem.text[:60]}...\" if len(elem.text) > 60 else f\"  - {type(elem).__name__}: {elem.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. LangChain Integration\n",
    "\n",
    "Unstructured integrates seamlessly with LangChain through the `langchain-unstructured` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Unstructured Integration\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import LangChain components\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "print(\"LangChain Unstructured Integration\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Loaded 276 LangChain documents\n",
      "\n",
      "First document:\n",
      "  Content: 4 2 0 2 c e D 9...\n",
      "  Metadata: {'source': 'sample_documents/docling_paper.pdf', 'coordinates': {'points': ((16.34, 216.16000000000008), (16.34, 308.36), (36.34, 308.36), (36.34, 216.16000000000008)), 'system': 'PixelSpace', 'layout_width': 612.0, 'layout_height': 792.0}, 'file_directory': 'sample_documents', 'filename': 'docling_paper.pdf', 'last_modified': '2025-12-04T13:44:43', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/pdf', 'category': 'UncategorizedText', 'element_id': '94656b2552b350a8a9bedcf9fe7ad9dc'}\n"
     ]
    }
   ],
   "source": [
    "# Basic UnstructuredLoader usage\n",
    "# Load a document and convert to LangChain Document objects\n",
    "\n",
    "# Load the sample HTML file\n",
    "loader = UnstructuredLoader(\n",
    "    file_path=\"sample_documents/docling_paper.pdf\",\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} LangChain documents\")\n",
    "print()\n",
    "\n",
    "# Show first document\n",
    "if docs:\n",
    "    print(\"First document:\")\n",
    "    print(f\"  Content: {docs[0].page_content[:200]}...\")\n",
    "    print(f\"  Metadata: {docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnstructuredLoader Modes:\n",
      "============================================================\n",
      "\n",
      "single:\n",
      "  Entire document as one Document object\n",
      "\n",
      "elements:\n",
      "  Each element as separate Document\n",
      "\n",
      "paged:\n",
      "  Each page as separate Document\n"
     ]
    }
   ],
   "source": [
    "# UnstructuredLoader modes\n",
    "# Different modes affect how documents are split\n",
    "\n",
    "print(\"UnstructuredLoader Modes:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "modes = {\n",
    "    \"single\": \"Entire document as one Document object\",\n",
    "    \"elements\": \"Each element as separate Document\",\n",
    "    \"paged\": \"Each page as separate Document\",\n",
    "}\n",
    "\n",
    "for mode, description in modes.items():\n",
    "    print(f\"\\n{mode}:\")\n",
    "    print(f\"  {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Documents in 'elements' mode: 276\n",
      "\n",
      "Sample documents:\n",
      "  [0] 4 2 0 2 c e D 9\n",
      "  [1] ] L C . s c [\n",
      "  [2] 5 v 9 6 8 9 0 . 8 0 4 2 : v i X r a\n",
      "  [3] Docling Technical Report\n",
      "  [4] Version 1.0\n"
     ]
    }
   ],
   "source": [
    "# Load in \"elements\" mode\n",
    "# Each element becomes a separate Document\n",
    "\n",
    "loader_elements = UnstructuredLoader(\n",
    "    file_path=\"sample_documents/docling_paper.pdf\",\n",
    "    mode=\"elements\",  # Each element as separate document\n",
    ")\n",
    "\n",
    "docs_elements = loader_elements.load()\n",
    "\n",
    "print(f\"Documents in 'elements' mode: {len(docs_elements)}\")\n",
    "print()\n",
    "\n",
    "print(\"Sample documents:\")\n",
    "for i, doc in enumerate(docs_elements[:5]):\n",
    "    content = doc.page_content[:80] + \"...\" if len(doc.page_content) > 80 else doc.page_content\n",
    "    print(f\"  [{i}] {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Documents in 'single' mode: 276\n",
      "\n",
      "Document length: 15 characters\n",
      "\n",
      "First 500 characters:\n",
      "Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial soft- ware, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\n"
     ]
    }
   ],
   "source": [
    "# Load in \"single\" mode\n",
    "# Entire document as one Document object\n",
    "\n",
    "loader_single = UnstructuredLoader(\n",
    "    file_path=\"sample_documents/docling_paper.pdf\",\n",
    "    mode=\"single\",  # Entire document as one\n",
    ")\n",
    "\n",
    "docs_single = loader_single.load()\n",
    "\n",
    "print(f\"Documents in 'single' mode: {len(docs_single)}\")\n",
    "print(f\"\\nDocument length: {len(docs_single[0].page_content)} characters\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(docs_single[11].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "PDF documents loaded: 276\n",
      "\n",
      "Document metadata examples:\n",
      "  Category: UncategorizedText\n",
      "  Page: 1\n",
      "  Content: 4 2 0 2 c e D 9...\n",
      "\n",
      "  Category: Title\n",
      "  Page: 1\n",
      "  Content: ] L C . s c [...\n",
      "\n",
      "  Category: UncategorizedText\n",
      "  Page: 1\n",
      "  Content: 5 v 9 6 8 9 0 . 8 0 4 2 : v i X r a...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UnstructuredLoader with strategy configuration\n",
    "# Pass strategy and other options to control partitioning\n",
    "\n",
    "pdf_path = \"sample_documents/docling_paper.pdf\"\n",
    "# Load PDF with hi_res strategy\n",
    "loader_pdf = UnstructuredLoader(\n",
    "    file_path=pdf_path,\n",
    "    mode=\"elements\",\n",
    "    strategy=\"fast\",  # Use fast for this demo\n",
    ")\n",
    "\n",
    "docs_pdf = loader_pdf.load()\n",
    "\n",
    "print(f\"PDF documents loaded: {len(docs_pdf)}\")\n",
    "print()\n",
    "\n",
    "# Show element types in metadata\n",
    "print(\"Document metadata examples:\")\n",
    "for doc in docs_pdf[:3]:\n",
    "    print(f\"  Category: {doc.metadata.get('category', 'N/A')}\")\n",
    "    print(f\"  Page: {doc.metadata.get('page_number', 'N/A')}\")\n",
    "    print(f\"  Content: {doc.page_content[:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 documents from sample_documents/sample.html\n",
      "Loaded 44 documents from sample_documents/sample.md\n",
      "\n",
      "Total documents: 64\n"
     ]
    }
   ],
   "source": [
    "# Load multiple files with UnstructuredLoader\n",
    "# You can load multiple documents at once\n",
    "import os \n",
    "# List of files to load\n",
    "files_to_load = [\n",
    "    \"sample_documents/sample.html\",\n",
    "    \"sample_documents/sample.md\",\n",
    "]\n",
    "\n",
    "# Load all files\n",
    "all_docs = []\n",
    "for file_path in files_to_load:\n",
    "    if os.path.exists(file_path):\n",
    "        loader = UnstructuredLoader(\n",
    "            file_path=file_path,\n",
    "            mode=\"elements\",\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "        print(f\"Loaded {len(docs)} documents from {file_path}\")\n",
    "\n",
    "print(f\"\\nTotal documents: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Complete RAG Application Example\n",
    "\n",
    "Let's build a complete RAG (Retrieval-Augmented Generation) application using:\n",
    "- Unstructured for document parsing\n",
    "- LangChain for the pipeline\n",
    "- ChromaDB for vector storage\n",
    "- OpenAI for embeddings and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.3.27\n",
      "Location: /Users/sourangshupal/Downloads/document-parsers-rag/.venv/lib/python3.12/site-packages\n",
      "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, pyyaml, requests, sqlalchemy\n",
      "Required-by: langchain-community\n"
     ]
    }
   ],
   "source": [
    "!uv pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv uninstall langchain\n",
    "!uv uninstall langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key found. Ready for RAG example.\n"
     ]
    }
   ],
   "source": [
    "# RAG Application Setup\n",
    "# Import all required components\n",
    "\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Verify API key is available\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"WARNING: OPENAI_API_KEY not set. RAG example will not work.\")\n",
    "    print(\"Set it in your .env file or environment variables.\")\n",
    "else:\n",
    "    print(\"OpenAI API key found. Ready for RAG example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: sample_documents/sample.html\n",
      "  -> Loaded 20 elements\n",
      "Loading: sample_documents/sample.md\n",
      "  -> Loaded 44 elements\n",
      "File not found: sample_documents/sample.pdf\n",
      "Loading: sample_documents/sample.docx\n",
      "  -> Loaded 53 elements\n",
      "Loading: sample_documents/sample.xlsx\n",
      "  -> Loaded 11 elements\n",
      "Loading: sample_documents/sample.pptx\n",
      "  -> Loaded 23 elements\n",
      "\n",
      "Total documents loaded: 151\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load documents using UnstructuredLoader\n",
    "# We'll use multiple document types to demonstrate flexibility\n",
    "\n",
    "def load_documents(file_paths):\n",
    "    \"\"\"\n",
    "    Load multiple documents using UnstructuredLoader.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of file paths to load\n",
    "    \n",
    "    Returns:\n",
    "        List of LangChain Document objects\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Loading: {file_path}\")\n",
    "            \n",
    "            loader = UnstructuredLoader(\n",
    "                file_path=file_path,\n",
    "                mode=\"elements\",  # Get individual elements\n",
    "                strategy=\"fast\",  # Fast processing\n",
    "            )\n",
    "            \n",
    "            docs = loader.load()\n",
    "            all_docs.extend(docs)\n",
    "            print(f\"  -> Loaded {len(docs)} elements\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    return all_docs\n",
    "\n",
    "# Load our sample documents\n",
    "document_paths = [\n",
    "    \"sample_documents/sample.html\",\n",
    "    \"sample_documents/sample.md\",\n",
    "    \"sample_documents/sample.pdf\",\n",
    "    \"sample_documents/sample.docx\",\n",
    "    \"sample_documents/sample.xlsx\",\n",
    "    \"sample_documents/sample.pptx\",\n",
    "]\n",
    "\n",
    "documents = load_documents(document_paths)\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents after splitting: 155\n",
      "\n",
      "Sample chunk:\n",
      "  Content: Document parsing is the process of analyzing and extracting structured information from various document formats. This includes PDFs, Word documents, HTML pages, and more....\n",
      "  Metadata: {'source': 'sample_documents/sample.html', 'last_modified': '2025-11-28T09:19:12', 'languages': ['eng'], 'file_directory': 'sample_documents', 'filename': 'sample.html', 'filetype': 'text/html', 'parent_id': '2a000fb58dc445e313c5dffe0ef3a614', 'category': 'NarrativeText', 'element_id': '0111c3838e668907531ba2247a8d97b1'}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Split documents into chunks\n",
    "# While UnstructuredLoader can chunk, we'll use LangChain's splitter for more control\n",
    "\n",
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Maximum chunk size\n",
    "    chunk_overlap=50,      # Overlap between chunks\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Documents after splitting: {len(split_docs)}\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(f\"  Content: {split_docs[1].page_content[:400]}...\")\n",
    "print(f\"  Metadata: {split_docs[1].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 155 documents\n",
      "Persisted to: ./chroma_db_unstructured\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create embeddings and vector store\n",
    "# Using OpenAI embeddings and Chroma for storage\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    # Initialize OpenAI embeddings\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-small\",  # Cost-effective embedding model\n",
    "    )\n",
    "    \n",
    "    # Create Chroma vector store\n",
    "    # persist_directory saves the database locally for reuse\n",
    "    split_docs = filter_complex_metadata(split_docs)\n",
    "        #documents=split_docs,\n",
    "        #metadata_keys=[\"source\", \"page\", \"chunk\"],\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=split_docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"./chroma_db_unstructured\",\n",
    "        collection_name=\"unstructured_rag_demo\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Vector store created with {len(split_docs)} documents\")\n",
    "    print(f\"Persisted to: ./chroma_db_unstructured\")\n",
    "else:\n",
    "    print(\"Skipping vector store creation - no API key\")\n",
    "    vectorstore = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is document parsing?\n",
      "\n",
      "Top 3 relevant chunks:\n",
      "============================================================\n",
      "\n",
      "Result 1:\n",
      "  Source: sample.html\n",
      "  Content: Document parsing is the process of analyzing and extracting structured information from various document formats. This includes PDFs, Word documents, HTML pages, and more....\n",
      "\n",
      "Result 2:\n",
      "  Source: sample.html\n",
      "  Content: Here's a simple example of using a document parser:...\n",
      "\n",
      "Result 3:\n",
      "  Source: sample.md\n",
      "  Content: Document Parsing Best Practices...\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Test similarity search\n",
    "# Verify the vector store is working correctly\n",
    "\n",
    "if vectorstore:\n",
    "    # Search for relevant documents\n",
    "    query = \"What is document parsing?\"\n",
    "    \n",
    "    results = vectorstore.similarity_search(\n",
    "        query=query,\n",
    "        k=3,  # Return top 3 results\n",
    "    )\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"\\nTop 3 relevant chunks:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"\\nResult {i+1}:\")\n",
    "        print(f\"  Source: {doc.metadata.get('filename', 'Unknown')}\")\n",
    "        print(f\"  Content: {doc.page_content[:400]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create the RAG chain\n",
    "# Combine retrieval with LLM for question answering\n",
    "\n",
    "if vectorstore and os.getenv(\"OPENAI_API_KEY\"):\n",
    "    # Initialize the LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",  # Cost-effective model\n",
    "        temperature=0,         # Deterministic responses\n",
    "    )\n",
    "    \n",
    "    # Create the retriever\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5},  # Retrieve top 5 chunks\n",
    "    )\n",
    "    \n",
    "    # Create the RAG chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",  # Stuff all context into prompt\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,  # Include sources in response\n",
    "    )\n",
    "    \n",
    "    print(\"RAG chain created successfully!\")\n",
    "else:\n",
    "    print(\"Cannot create RAG chain - missing dependencies\")\n",
    "    qa_chain = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the key benefits of document parsing?\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The key benefits of document parsing include:\n",
      "\n",
      "1. **Structured Information Extraction**: It allows for the extraction of structured data from unstructured documents, making it easier to analyze and utilize the information.\n",
      "\n",
      "2. **Improved Data Accessibility**: By converting unstructured data into a structured format, it enhances data accessibility and usability for various applications.\n",
      "\n",
      "3. **Automation of Data Processing**: Document parsing automates the process of data entry and processing, reducing manual effort and the potential for human error.\n",
      "\n",
      "4. **Enhanced Decision-Making**: With structured data readily available, organizations can make more informed decisions based on accurate and timely information.\n",
      "\n",
      "5. **Integration with AI Applications**: Document parsing is a critical component in modern AI applications, enabling better data handling and analysis.\n",
      "\n",
      "6. **Support for Various Formats**: It can handle multiple document formats, such as PDFs, Word documents, and HTML pages, making it versatile for different use cases. \n",
      "\n",
      "These benefits contribute to the overall efficiency and effectiveness of data management and analysis in various industries.\n",
      "\n",
      "Sources:\n",
      "  1. sample.md\n",
      "  2. sample.html\n",
      "  3. sample.html\n",
      "  4. sample.md\n",
      "  5. sample.html\n",
      "\n",
      "============================================================\n",
      "\n",
      "Question: What file formats are supported for document parsing?\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Document parsing supports various file formats, including PDFs, Word documents, HTML pages, and more.\n",
      "\n",
      "Sources:\n",
      "  1. sample.html\n",
      "  2. sample.html\n",
      "  3. sample.html\n",
      "  4. sample.md\n",
      "  5. sample.html\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Query the RAG system\n",
    "# Ask questions about the documents\n",
    "\n",
    "def ask_question(qa_chain, question):\n",
    "    \"\"\"\n",
    "    Ask a question using the RAG chain.\n",
    "    \n",
    "    Args:\n",
    "        qa_chain: The RetrievalQA chain\n",
    "        question: Question to ask\n",
    "    \n",
    "    Returns:\n",
    "        Answer and source documents\n",
    "    \"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    print(f\"\\nAnswer: {result['result']}\")\n",
    "    \n",
    "    print(f\"\\nSources:\")\n",
    "    for i, doc in enumerate(result['source_documents']):\n",
    "        source = doc.metadata.get('filename', 'Unknown')\n",
    "        print(f\"  {i+1}. {source}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Ask questions\n",
    "if qa_chain:\n",
    "    # Question 1\n",
    "    result1 = ask_question(\n",
    "        qa_chain, \n",
    "        \"What are the key benefits of document parsing?\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Question 2\n",
    "    result2 = ask_question(\n",
    "        qa_chain,\n",
    "        \"What file formats are supported for document parsing?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Interactive RAG Demo\n",
    "# A reusable function for asking questions\n",
    "\n",
    "def rag_query(question):\n",
    "    \"\"\"\n",
    "    Simplified function to query the RAG system.\n",
    "    \n",
    "    Args:\n",
    "        question: Your question about the documents\n",
    "    \n",
    "    Returns:\n",
    "        The answer string\n",
    "    \"\"\"\n",
    "    if not qa_chain:\n",
    "        return \"RAG chain not available. Please set OPENAI_API_KEY.\"\n",
    "    \n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    return result['result']\n",
    "\n",
    "# Example usage\n",
    "if qa_chain:\n",
    "    answer = rag_query(\"What are the applications of document parsing in RAG systems?\")\n",
    "    print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust error handling for document processing\n",
    "\n",
    "def safe_partition(file_path, **kwargs):\n",
    "    \"\"\"\n",
    "    Safely partition a document with error handling.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the document\n",
    "        **kwargs: Additional arguments for partition()\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (elements, error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return None, f\"File not found: {file_path}\"\n",
    "        \n",
    "        # Check file size (warn for large files)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        if file_size > 100 * 1024 * 1024:  # 100 MB\n",
    "            print(f\"Warning: Large file ({file_size / 1024 / 1024:.1f} MB)\")\n",
    "        \n",
    "        # Partition the document\n",
    "        elements = partition(filename=file_path, **kwargs)\n",
    "        \n",
    "        return elements, None\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        return None, \"File not found\"\n",
    "    except PermissionError:\n",
    "        return None, \"Permission denied\"\n",
    "    except Exception as e:\n",
    "        return None, f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "# Test with various scenarios\n",
    "test_files = [\n",
    "    \"sample_documents/sample.html\",\n",
    "    \"nonexistent_file.pdf\",\n",
    "    \"sample_documents/sample.md\",\n",
    "]\n",
    "\n",
    "print(\"Safe Partition Results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for file_path in test_files:\n",
    "    elements, error = safe_partition(file_path)\n",
    "    \n",
    "    if error:\n",
    "        print(f\"\\n{file_path}: ERROR - {error}\")\n",
    "    else:\n",
    "        print(f\"\\n{file_path}: SUCCESS - {len(elements)} elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Summary & Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Use the right strategy**:\n",
    "   - `fast` for quick processing of simple documents\n",
    "   - `hi_res` for complex layouts and accurate table extraction\n",
    "   - `ocr_only` for scanned documents\n",
    "\n",
    "2. **Choose appropriate chunking**:\n",
    "   - `chunk_by_title` preserves document structure\n",
    "   - Use overlap for better context in RAG\n",
    "   - Match chunk size to your embedding model's limits\n",
    "\n",
    "3. **Leverage metadata**:\n",
    "   - Page numbers for citations\n",
    "   - Element types for filtering\n",
    "   - File information for source tracking\n",
    "\n",
    "4. **Error handling**:\n",
    "   - Always wrap partition calls in try-except\n",
    "   - Check file existence and permissions\n",
    "   - Log errors for debugging\n",
    "\n",
    "### Recommended Workflow\n",
    "\n",
    "```python\n",
    "# 1. Load documents\n",
    "elements = partition(filename=\"document.pdf\", strategy=\"fast\")\n",
    "\n",
    "# 2. Filter relevant elements\n",
    "elements = [e for e in elements if len(e.text) > 10]\n",
    "\n",
    "# 3. Chunk for RAG\n",
    "chunks = chunk_by_title(elements, max_characters=1000)\n",
    "\n",
    "# 4. Create embeddings and store\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "\n",
    "# 5. Query with RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())\n",
    "result = qa_chain.invoke({\"query\": \"Your question\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Remove temporary files and vector store\n",
    "\n",
    "import shutil\n",
    "\n",
    "cleanup_paths = [\n",
    "    \"./chroma_db_unstructured\",\n",
    "    \"sample_documents/elements_export.json\",\n",
    "]\n",
    "\n",
    "print(\"Cleanup (optional):\")\n",
    "for path in cleanup_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  Would remove: {path}\")\n",
    "\n",
    "# Uncomment to actually clean up:\n",
    "# for path in cleanup_paths:\n",
    "#     if os.path.isdir(path):\n",
    "#         shutil.rmtree(path)\n",
    "#     elif os.path.isfile(path):\n",
    "#         os.remove(path)\n",
    "# print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources\n",
    "\n",
    "- [Unstructured Documentation](https://docs.unstructured.io/)\n",
    "- [Unstructured GitHub](https://github.com/Unstructured-IO/unstructured)\n",
    "- [LangChain Unstructured Integration](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file)\n",
    "- [Supported File Types](https://docs.unstructured.io/open-source/introduction/supported-file-types)\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook was created as a comprehensive guide to document parsing with Unstructured for RAG systems.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
